<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Principal Component Analysis | Cezar Sas</title>
    <link>http://sascezar.github.io/tags/principal-component-analysis/</link>
      <atom:link href="http://sascezar.github.io/tags/principal-component-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Principal Component Analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Cezar Sas - 2020</copyright><lastBuildDate>Thu, 30 Apr 2020 21:05:54 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Principal Component Analysis</title>
      <link>http://sascezar.github.io/tags/principal-component-analysis/</link>
    </image>
    
    <item>
      <title>Principal Component Analysis (PCA)</title>
      <link>http://sascezar.github.io/post/pca/</link>
      <pubDate>Thu, 30 Apr 2020 21:05:54 +0200</pubDate>
      <guid>http://sascezar.github.io/post/pca/</guid>
      <description>&lt;p&gt;In some situations, when working with machine learning models, we have to deal with high dimensional data. This brings various problems to the table, from higher computational time, to higer complexity required by the model to describe the data (check 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curse of Dimensionality&lt;/a&gt;). In order to solve this problem, various techniques to reduce the dimensionality of data have been proprosed, one of them is called Principal Component Analysis (PCA). PCA is a technique that can be used for different application, from dimensionality reduction and feature extraction to lossy data compression and data visualization. In this article, we will present the concepts behide this method, and then present two implementations in Python from scratch.&lt;/p&gt;
&lt;h4 id=&#34;description&#34;&gt;Description&lt;/h4&gt;
&lt;h4 id=&#34;formal-definition&#34;&gt;Formal Definition&lt;/h4&gt;
&lt;p&gt;For the formal definition, we will present the Singular Value Decomposition (SVD)formalization, as it is some stable and general compared to the Eigendecomposition, however, you can check the implementation section to get an idea of the differences.&lt;/p&gt;
&lt;p&gt;Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be the observation matrix. The first step is to center the data by removing the mean $\mathbf{X}&amp;rsquo; = \mathbf{M X}$, where $\mathbf{M}$ is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Centering_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Centering Matrix&lt;/a&gt;.
Then, we need to perform Singular Value Decomposition on $\mathbf{X}&#39;$, obtaining&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$\mathbf{X&amp;rsquo;} = \mathbf{U\Sigma V}^\top$$&lt;/p&gt;
&lt;p&gt;The covariance matrix $\mathbf{C}$ is given by:&lt;/p&gt;
&lt;p&gt;$$\mathbf{C} = \frac{1}{N-1}\mathbf{X&#39;^\top X&amp;rsquo;}$$
Which, by pluging in the SVD of $\mathbf{X}&#39;$, becomes:
$$\mathbf{C} = \frac{1}{N-1} \mathbf{V\Sigma U}^\top \mathbf{U\Sigma V}^\top$$
Since $\mathbf{U}$ is orthogonal:
$$\mathbf{C} = \frac{1}{N-1}\mathbf{V\Sigma}^2\mathbf{V}^\top$$&lt;/p&gt;
&lt;p&gt;The singular vectors $\mathbf{V}$ are the principal axes, sometimes they are alse refered to as principal diration or even principal components.  To get the principal components, also called principal component scores, we projet the input onto the principal axes:
$$\mathbf{X}&#39;\mathbf{V}^\top$$&lt;/p&gt;
&lt;p&gt;We can also rewrite the transformation as:
$$\mathbf{X}&#39;\mathbf{V} = \mathbf{U\Sigma V}^\top \mathbf{V} = \mathbf{U\Sigma}$$&lt;/p&gt;
&lt;p&gt;which is used when we want to apply the transformation to the same data we are fitting.&lt;/p&gt;
&lt;p&gt;The explained variance of each principal component is defined by:&lt;/p&gt;
&lt;p&gt;$$\sigma = \frac{\mathbf{\Sigma}^2}{N-1}$$&lt;/p&gt;
&lt;p&gt;To perform dimensionality reduction, to a size $k \lt d$, we then can pick a the first $k$ rows of $\mathbf{V}$, and to have the variance of these principal components a the first $k$ elements of the diagonal of $\sigma$. Then we can project $\mathbf{X}&#39;$ onto the reduced space using the new $\mathbf{V}$.&lt;/p&gt;
&lt;h4 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h4&gt;
&lt;h2 id=&#34;implementation-from-scratch&#34;&gt;Implementation from Scratch&lt;/h2&gt;
&lt;p&gt;For the implementation, we will create an abstract class with an interface similar to the one provided by the sklearn library (if you are not familiar with abstract classes check 
&lt;a href=&#34;https://www.geeksforgeeks.org/abstract-classes-in-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link). We will use this class as a base for our two solutions that use the previously described methods to compute the PCA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCA(ABC):
    def __init__(self, n_components):
        super().__init__()
        self._n_components = n_components
        self._components = None
        self._explained_variance = None
        self._mean = None

    @property
    def mean(self):
        return self._mean

    @property
    def explained_variance(self):
        return self._explained_variance

    @property
    def components(self):
        return self._components

    @abstractmethod
    def fit(self, X):
        pass

    def transform(self, X):
        if self._mean is not None:
            X -= self._mean
        X_transformed = np.dot(X, self._components.T)
        return X_transformed

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first implementation uses SVD, this method is more stable and is similar to the one used in sklearn.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCASVD(PCA):
    def fit(self, X, y=None, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape
        self._mean = np.mean(X, axis=0)
        X -= self._mean
        u, s, v = np.linalg.svd(X)
        self._components = v[:self._n_components]
        explained_variance = np.square(s[:self._n_components]) / (n - 1)
        self._explained_variance = explained_variance

        return self

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second implementation uses Eigenvalue Decomposition, as mentioned, this is less stable, as computing the dot product of $\mathbf{X}^\top\mathbf{X}$ in case of a numerically bad matrix (more formally, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Condition_number#Matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Condition Number&lt;/a&gt; of the matrix is high) the error will increase. This problem is avoided when using SVD since we don&amp;rsquo;t compute the covariance matrix directly from the observation matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCAEign(PCA):
    def fit(self, X):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape

        self._mean = np.mean(X, axis=0)
        X -= self._mean

        covariance = 1 / (n - 1) * np.matmul(X.T, X)
        Q, A = np.linalg.eig(covariance)
        components = A[:, :self._n_components]
        explained_variance = Q[:self._n_components]
        self._components = components.transpose()
        self._explained_variance = explained_variance.T

        return self
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca/134283#134283&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Relationship between SVD and Eigendecomposition in PCA&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
