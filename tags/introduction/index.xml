<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction | Cezar Sas</title>
    <link>http://sascezar.github.io/tags/introduction/</link>
      <atom:link href="http://sascezar.github.io/tags/introduction/index.xml" rel="self" type="application/rss+xml" />
    <description>Introduction</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Cezar Sas - 2020</copyright><lastBuildDate>Thu, 30 Apr 2020 22:19:46 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Introduction</title>
      <link>http://sascezar.github.io/tags/introduction/</link>
    </image>
    
    <item>
      <title>Principal Component Analysis (PCA)</title>
      <link>http://sascezar.github.io/post/pca/</link>
      <pubDate>Thu, 30 Apr 2020 22:19:46 +0200</pubDate>
      <guid>http://sascezar.github.io/post/pca/</guid>
      <description>&lt;p&gt;In some situations, when working with machine learning models, we have to deal with high dimensional data. This brings various problems to the table, from higher computational time, to higer complexity required by the model to describe the data (check 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curse of Dimensionality&lt;/a&gt;). In order to solve this problem, various techniques to reduce the dimensionality of data have been proposed, one of them is called Principal Component Analysis (PCA). PCA is a technique that can be used for different application, from dimensionality reduction and feature extraction to lossy data compression and data visualization. In this article, we will present the concepts behide this method, and then present two implementations in Python from scratch.&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Principal Component Anlysis is a linear transformation that transforms data into a new orthogonal space spanned by the vectors called principal components axes. These axes are found by searching for the vectors that model the higest variance in the data. The first axis is the one with the highest variance, the last one with the lowest variance.&lt;/p&gt;
&lt;h3 id=&#34;formal-definition&#34;&gt;Formal Definition&lt;/h3&gt;
&lt;p&gt;For the formal definition, we will present the Singular Value Decomposition (SVD) formalization, as it is some stable and general compared to the Eigendecomposition, however, you can check the implementation section to get an idea of the differences.&lt;/p&gt;
&lt;p&gt;Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be the observation matrix. The first step is to center the data by removing the mean $\mathbf{X}&amp;rsquo; = \mathbf{M X}$, where $\mathbf{M}$ is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Centering_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Centering Matrix&lt;/a&gt;. Then, we perform SVD on $\mathbf{X}&#39;$, obtaining&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$\mathbf{X&amp;rsquo;} = \mathbf{U\Sigma V}^\top$$&lt;/p&gt;
&lt;p&gt;The covariance matrix $\mathbf{C}$ is given by:&lt;/p&gt;
&lt;p&gt;$$\mathbf{C} = \frac{1}{N-1}\mathbf{X&#39;^\top X&amp;rsquo;}$$
which, by pluging in the SVD of $\mathbf{X}&#39;$, becomes:
$$\mathbf{C} = \frac{1}{N-1} \mathbf{V\Sigma U}^\top \mathbf{U\Sigma V}^\top$$
since $\mathbf{U}$ is orthogonal:
$$\mathbf{C} = \frac{1}{N-1}\mathbf{V\Sigma}^2\mathbf{V}^\top$$&lt;/p&gt;
&lt;p&gt;The singular vectors $\mathbf{V}$ are the principal axes, sometimes they are alse refered to as principal diration or even principal components.  To get the principal components, also called principal component scores, we projet the input onto the principal axes:
$$\mathbf{X}&#39;\mathbf{V}^\top$$&lt;/p&gt;
&lt;p&gt;We can also rewrite the transformation as:
$$\mathbf{X}&#39;\mathbf{V} = \mathbf{U\Sigma V}^\top \mathbf{V} = \mathbf{U\Sigma}$$&lt;/p&gt;
&lt;p&gt;which is used when we want to apply the transformation to the same data we are fitting.&lt;/p&gt;
&lt;p&gt;The explained variance of each principal component is defined by:&lt;/p&gt;
&lt;p&gt;$$\sigma = \frac{\mathbf{\Sigma}^2}{N-1}$$&lt;/p&gt;
&lt;p&gt;To perform dimensionality reduction, to a size $k \lt d$, we then can pick a the first $k$ rows of $\mathbf{V}$, and to have the variance of these principal components a the first $k$ elements of the diagonal of $\sigma$. Then we can project $\mathbf{X}&#39;$ onto the reduced space using the new $\mathbf{V}$.&lt;/p&gt;
&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;
&lt;h1 id=&#34;implementation-from-scratch&#34;&gt;Implementation from Scratch&lt;/h1&gt;
&lt;p&gt;For the implementation, we will create an abstract class with an interface similar to the one provided by the sklearn library (if you are not familiar with abstract classes check 
&lt;a href=&#34;https://www.geeksforgeeks.org/abstract-classes-in-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link). We will use this class as a base for our two solutions that use the previously described methods to compute the PCA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCA(ABC):
    def __init__(self, n_components):
        super().__init__()
        self._n_components = n_components
        self._components = None
        self._explained_variance = None
        self._mean = None

    @property
    def mean(self):
        return self._mean

    @property
    def explained_variance(self):
        return self._explained_variance

    @property
    def components(self):
        return self._components

    @abstractmethod
    def fit(self, X):
        pass

    def transform(self, X):
        if self._mean is not None:
            X -= self._mean
        X_transformed = np.dot(X, self._components.T)
        return X_transformed

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first implementation uses SVD, this method is more stable and is similar to the one used in sklearn.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCASVD(PCA):
    def fit(self, X, y=None, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape
        self._mean = np.mean(X, axis=0)
        X -= self._mean
        u, s, v = np.linalg.svd(X)
        self._components = v[:self._n_components]
        explained_variance = np.square(s[:self._n_components]) / (n - 1)
        self._explained_variance = explained_variance

        return self

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second implementation uses Eigenvalue Decomposition, as mentioned, this is less stable, as computing the dot product of $\mathbf{X}^\top\mathbf{X}$ in case of a numerically bad matrix (more formally, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Condition_number#Matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Condition Number&lt;/a&gt; of the matrix is high) the error will increase. This problem is avoided when using SVD since we don&amp;rsquo;t compute the covariance matrix directly from the observation matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCAEign(PCA):
    def fit(self, X):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape

        self._mean = np.mean(X, axis=0)
        X -= self._mean

        covariance = 1 / (n - 1) * np.matmul(X.T, X)
        Q, A = np.linalg.eig(covariance)
        components = A[:, :self._n_components]
        explained_variance = Q[:self._n_components]
        self._components = components.transpose()
        self._explained_variance = explained_variance.T

        return self
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca/134283#134283&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Relationship between SVD and Eigendecomposition in PCA&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>http://sascezar.github.io/post/neural-network/</link>
      <pubDate>Thu, 30 Apr 2020 16:03:47 +0200</pubDate>
      <guid>http://sascezar.github.io/post/neural-network/</guid>
      <description>&lt;p&gt;A Neural Network (NN) is a computational model inspired by networks of
biological neurons, wherein the neurons compute output values from inputs.
The simplest form of neural network is the feed forward neural
network, in which the information flows only in one direction, and there
is no recurrent connection between the nodes. Neural networks try to
approximate some function $f^{*}$ by defining a mapping $y = f(x; W)$
for an input $x$ by learning the value of the parameters $W$ that yield
the best function approximation. A neural network is called network
because it contains multiple layers of artificial neurons.
The artificial neuron is the building block of neural networks.&lt;/p&gt;
&lt;p&gt;A neuron is defined by an input $x$, a set of weights $W$, a bias $b$,
and an activation function $g$. The activation function should be
differentiable, since to actually learn, we need to compute derivatives.
With $x, W \in \mathbb{R}^{n\times1}$, $b \in \mathbb{R}$.
The output $y$ is defined as:&lt;/p&gt;
&lt;p&gt;$$y = g(W^{\intercal} x + b)$$&lt;/p&gt;
&lt;p&gt;Feed forward neural networks are composed by various layers, each layer
contains some neurons. The formal definition is very similar to the one
of the neuron. To simplify the notation, we include the bias term in the
weights matrix, and have an extra element $x_0 = 1$ in the input $x$.
Given a feed forward neural network with $L$ layers, the forward propagation is:&lt;/p&gt;
&lt;p&gt;$$y = f^{k}(W_{k}^{\intercal} * \dots f^{2}(W_{2}^{\intercal} * f^{1}(W_{1}^{\intercal} * x)) \dots)$$&lt;/p&gt;
&lt;p&gt;$$z^i = W^{i-1}a^{i-1}$$&lt;/p&gt;
&lt;p&gt;$$a^{i} = g(z^i)$$&lt;/p&gt;
&lt;p&gt;$$y = a^L g(z^L)$$&lt;/p&gt;
&lt;p&gt;Where $g$ is an activation function, $a^i$ is the activation of layer
$i$, and $a^0 = x$. The weight matrix $W_i$ controls the projection from
layer $i$ to layer $i+1$. Given a network with $s_i$ units in layer $i$,
and $s_{i+1}$ in layer $i+1$ the weight matrix
$W_i \in\mathbb{R}^{(s_{i+1})\times (s_i + 1)}$.&lt;/p&gt;
&lt;p&gt;As in other machine learning algorithms, we have to learn the parameters
of our neural network. We have a cost function, or objective function,
$J(\theta)$ that we try to minimize. Given a neural network $h_\theta$,
we compute our cost function as a sum of the error function
$\mathcal{L}$ for all the examples. The error function computes the
difference between the predicted value $\hat{y} = h_{\theta}(x)$ and the
actual value $y$. We also have a regularizer term $\Omega$ that is
weighted by the hyperparameter $\lambda$:&lt;/p&gt;
&lt;p&gt;$$\min_{\theta} J(\theta) = - \frac{1}{N} \sum_{1}^{N} \mathcal{L}(y_i, \hat{y_i}) + \lambda \Omega_\theta$$&lt;/p&gt;
&lt;p&gt;Now that we have defined what is the goal of our neural network, we can
start performing the training. The weights $\theta$ in the network are
the only parameters that can be modified to make the cost function $J$
as low as possible; thus we can minimize $J$ by using an iterative
process of gradient descent, for which we need to calculate the gradient
for the cost function with respect to the network parameters. As each
network consists of various layers, computing the gradient of the loss
function w.r.t the parameters $\frac{\partial J}{\partial \theta}$ is
non-trivial. To estimate the gradient, an algorithm called
backpropagation &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, from backward propagation of
errors, is used. As the name implies, backpropagation starts computing
the gradients from the output of the networks and moves backward to the
input through all layers. First, we need to compute the derivative of
$J(\theta)$ w.r.t the output, this will be our $\partial^{L}$, then go
backward:&lt;/p&gt;
&lt;p&gt;$$\partial^{L} = \frac{\partial}{\partial y} \mathcal{L}(y, \hat{y})$$&lt;/p&gt;
&lt;p&gt;$$\partial^i = {\theta^{i}}^\intercal \partial^{i+1} \odot \left(\frac{\partial}{\partial z^{i}} g(z^i)\right)$$&lt;/p&gt;
&lt;p&gt;For each layer, the computed gradients are then used to update the
corresponding parameters using 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradient descent&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. &amp;ldquo;
&lt;a href=&#34;https://www.nature.com/articles/323533a0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning representations by back-propagating errors.&lt;/a&gt;&amp;rdquo; Nature 323.6088 (1986): 533-536. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
