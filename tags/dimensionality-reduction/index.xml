<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dimensionality Reduction | Cezar Sas</title>
    <link>http://sascezar.github.io/tags/dimensionality-reduction/</link>
      <atom:link href="http://sascezar.github.io/tags/dimensionality-reduction/index.xml" rel="self" type="application/rss+xml" />
    <description>Dimensionality Reduction</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Cezar Sas - 2020</copyright><lastBuildDate>Thu, 30 Apr 2020 16:03:47 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Dimensionality Reduction</title>
      <link>http://sascezar.github.io/tags/dimensionality-reduction/</link>
    </image>
    
    <item>
      <title>Principal Component Analysis (PCA)</title>
      <link>http://sascezar.github.io/post/pca/</link>
      <pubDate>Thu, 30 Apr 2020 16:03:47 +0200</pubDate>
      <guid>http://sascezar.github.io/post/pca/</guid>
      <description>&lt;p&gt;Many times, when dealing with machine learning tasks, we have to deal with high dimensional data. However, this brings various problems, from higher computational time, to higer complexity required by the model
to describe the data (check 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curse of Dimensionality&lt;/a&gt;). In order to solve this problem, various techniques to reduce the dimensionality
of data have been proprosed. One of them is called Principal Component Analysis, and it&amp;rsquo;s a technique that van be used for different application, from dimensionality reduction and feature
extraction to lossy data compression and data visualization. In this article, we will present the concepts behide this method, and then present two implementations in Python from scratch.&lt;/p&gt;
&lt;h4 id=&#34;description&#34;&gt;Description&lt;/h4&gt;
&lt;h4 id=&#34;formal-definition&#34;&gt;Formal Definition&lt;/h4&gt;
&lt;p&gt;For the formal definition, we will focus on the Singular Value Decomposition formalization, as it is some stable and general compared to the Eigendecomposition, however, you can check the implementation section to get an idea of how .&lt;/p&gt;
&lt;p&gt;Let $\mathbf{X} \in \mathbb{R}^{(N \times D)}$ be the observation matrix. The first step is to center the data by removing the mean $\mathbf{X}&amp;rsquo; = C_D\mathbf{X}$, where $C_D$ is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Centering_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Centering Matrix&lt;/a&gt;.
Then, we need to perform Singular Value Decomposition on $\mathbf{X}&amp;lsquo;$, obtaining:&lt;/p&gt;
&lt;p&gt;$$\mathbf{X&amp;rsquo;} = \mathbf{USV}^\top$$&lt;/p&gt;
&lt;p&gt;The covariance matrix $\mathbf{C}$ is given by:&lt;/p&gt;
&lt;p&gt;$$\mathbf{C} = \frac{1}{N-1}\mathbf{X&#39;^\top X&amp;rsquo;}$$
$$\mathbf{C} = \frac{1}{N-1}\mathbf{U\Sigma V}^\top \mathbf{V\Sigma U}^\top$$
Since $\mathbf{V}$ is orthogonal:
$$\mathbf{C} = \frac{1}{N-1}\mathbf{U\Sigma}^2\mathbf{U}^\top$$&lt;/p&gt;
&lt;p&gt;The singular vectors $\mathbf{U}$ are the principal axes (sometimes they are alse refered to as principal components).
To get the principal components (sometimes called principal component scores), we projet the input onto the principal axes $\mathbf{X}&#39;\mathbf{U}$&lt;/p&gt;
&lt;h4 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h4&gt;
&lt;h2 id=&#34;implementation-from-scratch&#34;&gt;Implementation from Scratch&lt;/h2&gt;
&lt;p&gt;For the implementation, we will create an abstract class with an interface similar to the one provided by the sklearn library (if you are not familiar with abstract classes check 
&lt;a href=&#34;https://www.geeksforgeeks.org/abstract-classes-in-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link).
We will use this class as a base for our two classes that use the previously described methods to compute the PCA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCA(ABC):
    def __init__(self, n_components):
        super().__init__()
        self._n_components = n_components
        self._components = None
        self._explained_variance = None
        self._mean = None

    @property
    def mean(self):
        return self._mean

    @property
    def explained_variance(self):
        return self._explained_variance

    @property
    def components(self):
        return self._components

    @abstractmethod
    def fit(self, X):
        pass

    def transform(self, X):
        if self._mean is not None:
            X -= self._mean
        X_transformed = np.dot(X, self._components.T)
        return X_transformed

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first implementation is for the SVD method, this, is more stable and is similar to the one used in sklearn.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCASVD(PCA):
    def fit(self, X, y=None, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape
        self._mean = np.mean(X, axis=0)
        X -= self._mean
        v, s, u = np.linalg.svd(X)
        self._components = u[:self._n_components]
        explained_variance = np.square(s[:self._n_components]) / (n - 1)
        self._explained_variance = explained_variance

        return self

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second implementation is using Eigenvalue Decomposition.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCAEign(PCA):
    def fit(self, X):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape

        self._mean = np.mean(X, axis=0)
        X -= self._mean

        covariance = 1 / (n - 1) * np.matmul(X.T, X)
        Q, A = np.linalg.eig(covariance)
        components = A[:, :self._n_components]
        explained_variance = Q[:self._n_components]
        self._components = components.transpose()
        self._explained_variance = explained_variance.T

        return self
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
