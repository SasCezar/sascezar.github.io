<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dimensionality Reduction | Cezar Sas</title>
    <link>http://sascezar.github.io/tags/dimensionality-reduction/</link>
      <atom:link href="http://sascezar.github.io/tags/dimensionality-reduction/index.xml" rel="self" type="application/rss+xml" />
    <description>Dimensionality Reduction</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Cezar Sas - 2020</copyright><lastBuildDate>Thu, 30 Apr 2020 17:26:28 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Dimensionality Reduction</title>
      <link>http://sascezar.github.io/tags/dimensionality-reduction/</link>
    </image>
    
    <item>
      <title>Principal Component Analysis (PCA)</title>
      <link>http://sascezar.github.io/post/pca/</link>
      <pubDate>Thu, 30 Apr 2020 17:26:28 +0200</pubDate>
      <guid>http://sascezar.github.io/post/pca/</guid>
      <description>&lt;p&gt;In some situations, when working with machine learning models, we have to deal with high dimensional data. This brings various problems to the table, from higher computational time,
to higer complexity required by the model to describe the data (check 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curse of Dimensionality&lt;/a&gt;). In order to solve this problem,
various techniques to reduce the dimensionality of data have been proprosed. One of them is called Principal Component Analysis, and it&amp;rsquo;s a technique that van be used for different application,
from dimensionality reduction and feature extraction to lossy data compression and data visualization. In this article, we will present the concepts behide this method,
and then present two implementations in Python from scratch.&lt;/p&gt;
&lt;h4 id=&#34;description&#34;&gt;Description&lt;/h4&gt;
&lt;h4 id=&#34;formal-definition&#34;&gt;Formal Definition&lt;/h4&gt;
&lt;p&gt;For the formal definition, we will focus on the Singular Value Decomposition formalization, as it is some stable and general compared to the Eigendecomposition, however, you can check the implementation section to get an idea of the differences.&lt;/p&gt;
&lt;p&gt;Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be the observation matrix. The first step is to center the data by removing the mean $\mathbf{X}&amp;rsquo; = \mathbf{M X}$, where $\mathbf{M}$ is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Centering_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Centering Matrix&lt;/a&gt;.
Then, we need to perform Singular Value Decomposition on $\mathbf{X}&#39;$, obtaining:&lt;/p&gt;
&lt;p&gt;$$\mathbf{X&amp;rsquo;} = \mathbf{U\Sigma V}^\top$$&lt;/p&gt;
&lt;p&gt;The covariance matrix $\mathbf{C}$ is given by:&lt;/p&gt;
&lt;p&gt;$$\mathbf{C} = \frac{1}{N-1}\mathbf{X&#39;^\top X&amp;rsquo;}$$
Which, by pluging in the SVD of $\mathbf{X}&#39;$ becomes:
$$\mathbf{C} = \frac{1}{N-1} \mathbf{V\Sigma U}^\top \mathbf{U\Sigma V}^\top$$
Since $\mathbf{U}$ is orthogonal:
$$\mathbf{C} = \frac{1}{N-1}\mathbf{V\Sigma}^2\mathbf{V}^\top$$&lt;/p&gt;
&lt;p&gt;The singular vectors $\mathbf{V}$ are the principal axes, sometimes they are alse refered to as principal components.
To get the principal components, also called principal component scores, we projet the input onto the principal axes:
$$\mathbf{X}&#39;\mathbf{V}^\top$$&lt;/p&gt;
&lt;!--$$\mathbf{X}&#39;\mathbf{V} = \mathbf{U\Sigma V}^\top \mathbf{V} = \mathbf{U\Sigma}$$ --&gt;
&lt;p&gt;The explained variance of each principal component is defined by:&lt;/p&gt;
&lt;p&gt;$$\sigma = \frac{\mathbf{\Sigma}^2}{N-1}$$&lt;/p&gt;
&lt;p&gt;To perform dimensionality reduction, to a size $k \lt d$, we then can pick a the first $k$ rows of $\mathbf{V}$, and to have the variance of these principal components a the first $k$ elements of the diagonal of $\sigma$.&lt;/p&gt;
&lt;h4 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h4&gt;
&lt;h2 id=&#34;implementation-from-scratch&#34;&gt;Implementation from Scratch&lt;/h2&gt;
&lt;p&gt;For the implementation, we will create an abstract class with an interface similar to the one provided by the sklearn library (if you are not familiar with abstract classes check 
&lt;a href=&#34;https://www.geeksforgeeks.org/abstract-classes-in-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link).
We will use this class as a base for our two classes that use the previously described methods to compute the PCA.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCA(ABC):
    def __init__(self, n_components):
        super().__init__()
        self._n_components = n_components
        self._components = None
        self._explained_variance = None
        self._mean = None

    @property
    def mean(self):
        return self._mean

    @property
    def explained_variance(self):
        return self._explained_variance

    @property
    def components(self):
        return self._components

    @abstractmethod
    def fit(self, X):
        pass

    def transform(self, X):
        if self._mean is not None:
            X -= self._mean
        X_transformed = np.dot(X, self._components.T)
        return X_transformed

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first implementation uses SVD, this method is more stable and is similar to the one used in sklearn.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCASVD(PCA):
    def fit(self, X, y=None, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape
        self._mean = np.mean(X, axis=0)
        X -= self._mean
        v, s, u = np.linalg.svd(X)
        self._components = u[:self._n_components]
        explained_variance = np.square(s[:self._n_components]) / (n - 1)
        self._explained_variance = explained_variance

        return self

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second implementation uses Eigenvalue Decomposition, as mentioned, this is less stable, as computing the
dot product of $\mathbf{X}^\top\mathbf{X}$ in case of a numerically bad matrix (more formally, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Condition_number#Matrices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Condition Number&lt;/a&gt; of the matrix is high) the error will increase.
This problem is aoided when using the SVD, as we don&amp;rsquo;t compute the covariance matrix directly from the observation matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class PCAEign(PCA):
    def fit(self, X):
        &amp;quot;&amp;quot;&amp;quot;
        Finds the principal axes and their explained variance
        :param X: Numpy array
        :return:
        &amp;quot;&amp;quot;&amp;quot;
        n, _ = X.shape

        self._mean = np.mean(X, axis=0)
        X -= self._mean

        covariance = 1 / (n - 1) * np.matmul(X.T, X)
        Q, A = np.linalg.eig(covariance)
        components = A[:, :self._n_components]
        explained_variance = Q[:self._n_components]
        self._components = components.transpose()
        self._explained_variance = explained_variance.T

        return self
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
